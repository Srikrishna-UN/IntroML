{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c7bdce40",
   "metadata": {
    "id": "c7bdce40"
   },
   "source": [
    "### Activity 3.2_ Gradient Descent Implementation\n",
    "\n",
    "Student USN - [#[Write your USN here]]\n",
    "Student Name - [#[Write your Name here]]\n",
    "\n",
    "**Prepared by:**\n",
    "\n",
    "Prof. Ashwini M, Prof. Shabbeer B, Prof. Sohbana P, Prof Shanti K and Ms. Achala H<br>\n",
    "SoCSE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50700540",
   "metadata": {
    "id": "50700540"
   },
   "source": [
    "## What is Gradient Decent?\n",
    "\n",
    "Gradient descent is an optimization algorithm commonly used in machine learning and other fields to minimize a function.\n",
    "\n",
    "\n",
    "### **Key points about gradient descent:**\n",
    "\n",
    "- It's an iterative algorithm, meaning it takes small steps repeatedly.\n",
    "- It needs a function to optimize, like a loss function in machine learning.\n",
    "- It requires a learning rate, which controls the size of the steps it takes.\n",
    "- It's not guaranteed to find the global minimum, but it usually finds a good local minimum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f5d9cbd6",
   "metadata": {},
   "outputs": [
    {
     "ename": "ArgumentError",
     "evalue": "ArgumentError: Package numpy not found in current path.\n- Run `import Pkg; Pkg.add(\"numpy\")` to install the numpy package.",
     "output_type": "error",
     "traceback": [
      "ArgumentError: Package numpy not found in current path.\n",
      "- Run `import Pkg; Pkg.add(\"numpy\")` to install the numpy package.\n",
      "\n",
      "Stacktrace:\n",
      "  [1] macro expansion\n",
      "    @ .\\loading.jl:1163 [inlined]\n",
      "  [2] macro expansion\n",
      "    @ .\\lock.jl:223 [inlined]\n",
      "  [3] require(into::Module, mod::Symbol)\n",
      "    @ Base .\\loading.jl:1144\n",
      "  [4] eval\n",
      "    @ .\\boot.jl:368 [inlined]\n",
      "  [5] include_string(mapexpr::typeof(REPL.softscope), mod::Module, code::String, filename::String)\n",
      "    @ Base .\\loading.jl:1428\n",
      "  [6] #invokelatest#2\n",
      "    @ .\\essentials.jl:729 [inlined]\n",
      "  [7] invokelatest\n",
      "    @ .\\essentials.jl:726 [inlined]\n",
      "  [8] (::VSCodeServer.var\"#208#209\"{VSCodeServer.NotebookRunCellArguments, String})()\n",
      "    @ VSCodeServer c:\\Users\\Srikrishna U N\\.vscode\\extensions\\julialang.language-julia-1.66.2\\scripts\\packages\\VSCodeServer\\src\\serve_notebook.jl:19\n",
      "  [9] withpath(f::VSCodeServer.var\"#208#209\"{VSCodeServer.NotebookRunCellArguments, String}, path::String)\n",
      "    @ VSCodeServer c:\\Users\\Srikrishna U N\\.vscode\\extensions\\julialang.language-julia-1.66.2\\scripts\\packages\\VSCodeServer\\src\\repl.jl:274\n",
      " [10] notebook_runcell_request(conn::VSCodeServer.JSONRPC.JSONRPCEndpoint{Base.PipeEndpoint, Base.PipeEndpoint}, params::VSCodeServer.NotebookRunCellArguments)\n",
      "    @ VSCodeServer c:\\Users\\Srikrishna U N\\.vscode\\extensions\\julialang.language-julia-1.66.2\\scripts\\packages\\VSCodeServer\\src\\serve_notebook.jl:13\n",
      " [11] dispatch_msg(x::VSCodeServer.JSONRPC.JSONRPCEndpoint{Base.PipeEndpoint, Base.PipeEndpoint}, dispatcher::VSCodeServer.JSONRPC.MsgDispatcher, msg::Dict{String, Any})\n",
      "    @ VSCodeServer.JSONRPC c:\\Users\\Srikrishna U N\\.vscode\\extensions\\julialang.language-julia-1.66.2\\scripts\\packages\\JSONRPC\\src\\typed.jl:67\n",
      " [12] serve_notebook(pipename::String, outputchannel_logger::Base.CoreLogging.SimpleLogger; crashreporting_pipename::String)\n",
      "    @ VSCodeServer c:\\Users\\Srikrishna U N\\.vscode\\extensions\\julialang.language-julia-1.66.2\\scripts\\packages\\VSCodeServer\\src\\serve_notebook.jl:139\n",
      " [13] top-level scope\n",
      "    @ c:\\Users\\Srikrishna U N\\.vscode\\extensions\\julialang.language-julia-1.66.2\\scripts\\notebook\\notebook.jl:32"
     ]
    }
   ],
   "source": [
    "#import numpy libray for given task\n",
    "#[Write your code here]\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ea86eb45",
   "metadata": {
    "id": "ea86eb45"
   },
   "outputs": [
    {
     "ename": "UndefVarError",
     "evalue": "UndefVarError: np not defined",
     "output_type": "error",
     "traceback": [
      "UndefVarError: np not defined\n",
      "\n",
      "Stacktrace:\n",
      " [1] top-level scope\n",
      "   @ c:\\Sem\\IML\\IntroML\\CO3_Activity_3_2_Gradient_Decent Student.ipynb:2"
     ]
    }
   ],
   "source": [
    "# Define sample data\n",
    "x = np.array([1, 2, 3, 4, 5])\n",
    "y = np.array([2, 4, 5, 4, 5])\n",
    "\n",
    "# Define a simple linear model [ w * x + b ]\n",
    "#[Write your code here]\n",
    "def model(x,w,b):\n",
    "    return (w*x)+b\n",
    "\n",
    "# Define the mean squared error loss function\n",
    "#[Write your code here]\n",
    "def MSE(y_pred,y)\n",
    "    return np.mean((y-y_pred)**2)\n",
    "\n",
    "# Set initial parameters [w = 0 and b = 0 ]\n",
    "#[Write your code here]\n",
    "w = 0\n",
    "b = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bac5219",
   "metadata": {
    "id": "7bac5219"
   },
   "source": [
    "### Implementation of Gradient Decent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e401f32",
   "metadata": {
    "id": "0e401f32",
    "outputId": "e3f78f15-9288-44a1-8c77-eb762b24b18b"
   },
   "outputs": [],
   "source": [
    "# Set the Learning_rate 0.01\n",
    "#[Write your code here]\n",
    "lr = 0.01\n",
    "# Gradient descent loop [for 100 iteration]\n",
    "for _ in range(100)\n",
    "#[Write your code here]\n",
    "    y_pred = model(x,w,b)\n",
    "    loss = MSE(y_pred,y)\n",
    "    # Calculate gradients\n",
    "    #[Write your code here]\n",
    "    dw = np.mean(2*x*(y_pred-y))\n",
    "    db = np.mean(2*(y_pred-y))\n",
    "    # Update parameters\n",
    "    #[Write your code here]\n",
    "    w -= lr*dw\n",
    "    b -= lr*db\n",
    "\n",
    "    print(\"Iteration:\", i, \"Loss:\", loss)\n",
    "\n",
    "# Print final parameters and loss\n",
    "print(\"Final w:\", w)\n",
    "print(\"Final b:\", b)\n",
    "print(\"Final loss:\", loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bf3c010",
   "metadata": {
    "id": "5bf3c010",
    "outputId": "a55754b1-a712-421e-8835-e9a6eed9f03e"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Plot initial data and line\n",
    "plt.scatter(x, y, label='Data points')\n",
    "plt.plot(x, model(x, w, b), color='red', label='Initial line')\n",
    "\n",
    "# Gradient descent loop\n",
    "for i in range(100):\n",
    "    y_pred = model(x, w, b)\n",
    "    loss = mean_squared_error(y, y_pred)\n",
    "\n",
    "    # Calculate gradients\n",
    "    dw = np.mean(2 * x * (y_pred - y))\n",
    "    db = np.mean(2 * (y_pred - y))\n",
    "\n",
    "    # Update parameters\n",
    "    w = w - learning_rate * dw\n",
    "    b = b - learning_rate * db\n",
    "\n",
    "    # Plot updated line\n",
    "    if i % 10 == 0:  # Plot every 10 iterations for better visualization\n",
    "        plt.plot(x, model(x, w, b), color='green', alpha=0.3)\n",
    "\n",
    "# Print final parameters and loss\n",
    "print(\"Final w:\", w)\n",
    "print(\"Final b:\", b)\n",
    "print(\"Final loss:\", loss)\n",
    "\n",
    "# Plot final line\n",
    "plt.plot(x, model(x, w, b), color='blue', label='Final line')\n",
    "\n",
    "# Add labels and legend\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('Y')\n",
    "plt.legend()\n",
    "plt.title('Gradient Descent Visualization')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b24ef5e5",
   "metadata": {
    "id": "b24ef5e5"
   },
   "source": [
    "## Class activity excercise : [Intermediate Level ]\n",
    "\n",
    "Give the short inference via experiment by changing the learning rates. Write your understanding in two to three lines.\n",
    "\n",
    "#[Write your inference here in this markdown text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4260d5df",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Just execute the cell after completing the above tasks. [For Authencation of the submission]\n",
    "import getpass\n",
    "from datetime import datetime\n",
    "\n",
    "username = getpass.getuser()\n",
    "timestamp = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "print(f\"Username: {username}\")\n",
    "print(f\"Timestamp: {timestamp}\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Julia 1.8.5",
   "language": "julia",
   "name": "julia-1.8"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
